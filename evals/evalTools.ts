/**
 * Evaluation Tools for AI Agent Testing
 * 
 * This module provides utilities for running, scoring, and persisting
 * evaluation results for AI agent experiments.
 */

// Load environment variables from .env file
import "dotenv/config";
// Import types from autoevals library for scoring
import type { Score, Scorer } from "autoevals";
// Import chalk for colorized console output
import chalk from "chalk";
// Import lowdb for JSON-based data persistence
import { JSONFilePreset } from "lowdb/node";

/**
 * Represents a single evaluation run
 * 
 * @property input - The input provided to the task
 * @property output - The output generated by the AI agent
 * @property expected - The expected output or ground truth
 * @property scores - Array of scores from different evaluation metrics
 * @property createdAt - Optional timestamp when the run was created
 */
type Run = {
  input: any;
  output: any;
  expected: any;
  scores: {
    name: Score["name"];
    score: Score["score"];
  }[];
  createdAt?: string;
};

/**
 * Represents a collection of evaluation runs
 * 
 * @property runs - Array of individual evaluation runs
 * @property score - The average score across all runs in the set
 * @property createdAt - Timestamp when the set was created
 */
type Set = {
  runs: Run[];
  score: number;
  createdAt: string;
};

/**
 * Represents an experiment containing multiple evaluation sets
 * 
 * @property name - The name of the experiment
 * @property sets - Array of evaluation sets, typically representing different versions or iterations
 */
type Experiment = {
  name: string;
  sets: Set[];
};

/**
 * Top-level data structure for storing all experiments
 * 
 * @property experiments - Array of all experiments
 */
type Data = {
  experiments: Experiment[];
};

/**
 * Default data structure used when initializing a new database
 */
const defaultData: Data = {
  experiments: [],
};

/**
 * Initializes and returns a connection to the JSON database
 * 
 * @returns A Promise resolving to the database instance
 */
const getDb = async () => {
  const db = await JSONFilePreset<Data>("results.json", defaultData);
  return db;
};

/**
 * Calculates the average score across all runs
 * 
 * First calculates the average score for each run (across all scorers),
 * then calculates the average of these averages.
 * 
 * @param runs - Array of evaluation runs
 * @returns The calculated average score
 */
const calculateAvgScore = (runs: Run[]) => {
  const totalScores = runs.reduce((sum, run) => {
    const runAvg =
      run.scores.reduce((sum, score) => sum + score.score, 0) /
      run.scores.length;
    return sum + runAvg;
  }, 0);
  return totalScores / runs.length;
};

/**
 * Loads an experiment by name from the database
 * 
 * @param experimentName - The name of the experiment to load
 * @returns A Promise resolving to the experiment if found, or undefined if not found
 */
export const loadExperiment = async (
  experimentName: string
): Promise<Experiment | undefined> => {
  const db = await getDb();
  return db.data.experiments.find((e) => e.name === experimentName);
};

/**
 * Saves a new evaluation set to an experiment
 * 
 * If the experiment doesn't exist, it creates a new one.
 * Adds timestamps to runs and calculates the average score for the set.
 * 
 * @param experimentName - The name of the experiment to save to
 * @param runs - Array of evaluation runs without timestamps
 */
export const saveSet = async (
  experimentName: string,
  runs: Omit<Run, "createdAt">[]
) => {
  const db = await getDb();

  // Add timestamps to each run
  const runsWithTimestamp = runs.map((run) => ({
    ...run,
    createdAt: new Date().toISOString(),
  }));

  // Create a new evaluation set with calculated average score
  const newSet = {
    runs: runsWithTimestamp,
    score: calculateAvgScore(runsWithTimestamp),
    createdAt: new Date().toISOString(),
  };

  // Find existing experiment or create a new one
  const existingExperiment = db.data.experiments.find(
    (e) => e.name === experimentName
  );

  if (existingExperiment) {
    existingExperiment.sets.push(newSet);
  } else {
    db.data.experiments.push({
      name: experimentName,
      sets: [newSet],
    });
  }

  // Persist changes to the database
  await db.write();
};

/**
 * Runs a complete evaluation experiment
 * 
 * Executes the task on each input, scores the outputs using the provided scorers,
 * compares with previous results, and saves the new results.
 * 
 * @param experiment - The name of the experiment
 * @param task - The async function that processes inputs and returns outputs
 * @param data - Array of test cases with inputs and expected outputs
 * @param scorers - Array of scoring functions to evaluate the outputs
 * @returns Array of evaluation results
 */
export const runEval = async <T = any>(
  experiment: string,
  {
    task,
    data,
    scorers,
  }: {
    task: (input: any) => Promise<T>;
    data: { input: any; expected?: T; reference?: string | string[] }[];
    scorers: Scorer<T, any>[];
  }
) => {
  // Process each test case in parallel
  const results = await Promise.all(
    data.map(async ({ input, expected, reference }) => {
      // Execute the task with the given input
      const results = await task(input);
      let context: string | string[];
      let output: string;

      // Handle different result formats (with or without context)
      if (results.context) {
        // Format with separate context and response
        context = results.context;
        output = results.response;
      } else {
        // Simple format with just the output
        output = results;
      }

      // Apply all scorers in parallel
      const scores = await Promise.all(
        scorers.map(async (scorer) => {
          // Score the output using the current scorer
          const score = await scorer({
            input,
            output: results,
            expected,
            reference,
            context,
          });
          // Extract just the name and score value
          return {
            name: score.name,
            score: score.score,
          };
        })
      );

      // Construct the final result for this test case
      const result = {
        input,
        output,
        expected,
        scores,
      };

      return result;
    })
  );

  // Load previous experiment results for comparison
  const previousExperiment = await loadExperiment(experiment);
  // Get the score from the most recent evaluation set, or default to 0
  const previousScore =
    previousExperiment?.sets[previousExperiment.sets.length - 1]?.score || 0;
  // Calculate the average score for the current results
  const currentScore = calculateAvgScore(results);
  // Calculate the difference to show improvement or regression
  const scoreDiff = currentScore - previousScore;

  // Determine the color for console output based on score comparison
  // Green for improvement, red for regression, blue for no change or new experiment
  const color = previousExperiment
    ? scoreDiff > 0
      ? chalk.green
      : scoreDiff < 0
      ? chalk.red
      : chalk.blue
    : chalk.blue;

  // Display the evaluation results in the console
  console.log(`Experiment: ${experiment}`);
  console.log(`Previous score: ${color(previousScore.toFixed(2))}`);
  console.log(`Current score: ${color(currentScore.toFixed(2))}`);
  console.log(
    `Difference: ${scoreDiff > 0 ? "+" : ""}${color(scoreDiff.toFixed(2))}`
  );
  console.log();

  // Save the evaluation results to the database
  await saveSet(experiment, results);

  // Return the results for further processing if needed
  return results;
};
